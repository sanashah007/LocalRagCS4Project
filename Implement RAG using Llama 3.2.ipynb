{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, CSVLoader, UnstructuredWordDocumentLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#from htmlTemplate import css, bot_template, user_template\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83e771c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory=\"data_directory\" #the directory where documents are stored.\n",
    "embedding_model='sentence-transformers/all-MiniLM-L6-v2' #the model identifier for Sentence Transformers, which will be used to generate text embeddings.\n",
    "llm_model =\"llama3.2\" #specifies the version of the llm model used for generating responses in a chatbot setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_split_docs(directory):\n",
    "    # Load the documents\n",
    "    loaders = [\n",
    "        DirectoryLoader(directory, glob=\"**/*.pdf\",show_progress=True, loader_cls=PyPDFLoader),\n",
    "        DirectoryLoader(directory, glob=\"**/*.txt\",show_progress=True, loader_cls=TextLoader),\n",
    "        DirectoryLoader(directory, glob=\"**/*.docx\",show_progress=True),\n",
    "        DirectoryLoader(directory, glob=\"**/*.csv\",loader_cls=CSVLoader)\n",
    "    ]\n",
    "\n",
    "\n",
    "    documents=[]\n",
    "    for loader in loaders:\n",
    "        data =loader.load()\n",
    "        documents.extend(data)\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,  \n",
    "        chunk_overlap=256,\n",
    "        disallowed_special=(),\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "\n",
    "    # Split the documents and keep metadata\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba2d9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "def ingest_into_vectordb(split_docs):\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    print(\"Documents are inserted into FAISS vectorstore\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_conversation_chain(retriever):\n",
    "    llm = Ollama(model=llm_model)\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given the chat history and the latest user question, \"\n",
    "        \"provide a response that directly addresses the user's query based on the provided documents. \"\n",
    "        \"Do not rephrase the question or ask follow-up questions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    ### Answer question ###\n",
    "    system_prompt = (\n",
    "        \"As a personal chat assistant, provide accurate and relevant information based on the provided document in 2-3 sentences. \"\n",
    "      \n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "    ### Statefully manage chat history ###\n",
    "    store = {}\n",
    "\n",
    "\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    print(\"Conversational chain created\")\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      " 98%|█████████▊| 50/51 [00:00<00:00, 8852.48it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 50 passages\n",
      "Documents are inserted into FAISS vectorstore\n",
      "Conversational chain created\n"
     ]
    }
   ],
   "source": [
    " # \"Answer should be limited to 50 words and 2-3 sentences.  do not prompt to select answers or do not formualate a stand alone question. do not ask questions in the response. \"\n",
    "split_docs=prepare_and_split_docs(file_directory)\n",
    "vector_db= ingest_into_vectordb(split_docs)\n",
    "retriever =vector_db.as_retriever()\n",
    "conversational_rag_chain=get_conversation_chain(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e513bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question=\"Who lives in Kansas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided document, Justin Xia lives in Kansas with Zach's grandmother. Unfortunately, the document does not provide information on Zach's family members or their relationship to Justin Xia beyond that.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa1=conversational_rag_chain.invoke(\n",
    "    {\"input\": user_question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")\n",
    "print(qa1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194d531",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cs4Project",
   "language": "python",
   "name": "cs4project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
